{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Content\n",
        "- AdaBoost\n",
        "- Cascading"
      ],
      "metadata": {
        "id": "Tu4-PtrM0HDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AdaBoost**"
      ],
      "metadata": {
        "id": "OEk0gCgD3S0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*DwvwMlOcT1T9hZwIJvMfng.png\" >"
      ],
      "metadata": {
        "id": "xbwNkkfKcW_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBost means Adaptive Boosting \n",
        "\n",
        "\n",
        "#### What does AdaBoost do?\n",
        "\n",
        "Let's try to understand it using simple example (2-D data)\n",
        "\n",
        "Let us assume a toy dataset with negatives and positives\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5E_BPuyA47o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Og9gvnSUpx3uqaS12NUybZatrwLWhIqZ' >\n"
      ],
      "metadata": {
        "id": "i8z4mHadE0aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we build a weak classifier i.e. which is a horizontal or a vertical plane \n",
        "\n",
        "Here,\n",
        "* $ϵ$ is the error of the model\n",
        "\n",
        "* $α$ is how good our model is, which is considered as the weight of the model"
      ],
      "metadata": {
        "id": "Z2gPo0GhG1qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1e17PWvIvvhH0NeL_hAB45iFI8pBZAnPt' >\n",
        "\n"
      ],
      "metadata": {
        "id": "Ne6WGRK1FuMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building the first model, \n",
        "\n",
        "we give more weightage to the erronous data while building the second model \n",
        "\n",
        "\n",
        "Now as more weightage is given to the erronous data,\n",
        "- the second model will be based on the erronous data.\n"
      ],
      "metadata": {
        "id": "nkIJjNgUE0tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1EauFehFWBMZB7scQaBKgv9bcTYwv9N_f' >\n",
        "\n"
      ],
      "metadata": {
        "id": "teuVjrawE4-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again after the second model,\n",
        "\n",
        "there will be few erronous points in the classification, so these points will be given more weightage while building the next model \n",
        "\n",
        "#### What is it doing intuitivelty?\n",
        "Here what we are doing is giving weights to points proportional to the errors they are making\n",
        "\n",
        "So, to get final model, we multiply each model with a weight.\n",
        "\n",
        "The output will be sign of this multiplication i.e. +ve/ -ve"
      ],
      "metadata": {
        "id": "xDiKJIy0E5Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1PX8ikROK910A_ms432Wfm6CB0vt7yg9H' >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEz_g03eE7Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drawbacks of Adaboost\n",
        "\n",
        "Major drawback of Adaboost is that \n",
        "- unlike GBDT, we don't have flexibility to use any loss function."
      ],
      "metadata": {
        "id": "gS1zL1yIISSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code walkthrough"
      ],
      "metadata": {
        "id": "GWS3y1-cctDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "model2 = AdaBoostClassifier(DTC(max_depth = 5))\n",
        "\n",
        "params= {\n",
        "    \"n_estimators\" : [50, 100, 150],\n",
        "    \"learning_rate\" : [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(model2, params, cv = 5, scoring = \"accuracy\")\n",
        "clf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "pFHJxSIBcg_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396fbc6e-298b-4e81-f2cc-334f57cd5aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5)),\n",
              "             param_grid={'learning_rate': [0.1, 0.2, 0.3],\n",
              "                         'n_estimators': [50, 100, 150]},\n",
              "             scoring='accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = clf.cv_results_\n",
        "\n",
        "for i in range(len(res[\"params\"])):\n",
        "  print(f\"Parameters: {res['params'][i]}\\tMean_score: {res['mean_test_score'][i]}\\tRank: {res['rank_test_score'][i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM7C0VVEDuns",
        "outputId": "728314bd-1c09-40bd-de25-9ffa7400bd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: {'learning_rate': 0.1, 'n_estimators': 50}\tMean_score: 0.8614918585655154\tRank: 8\n",
            "Parameters: {'learning_rate': 0.1, 'n_estimators': 100}\tMean_score: 0.8719566331830402\tRank: 6\n",
            "Parameters: {'learning_rate': 0.1, 'n_estimators': 150}\tMean_score: 0.8739227127348593\tRank: 5\n",
            "Parameters: {'learning_rate': 0.2, 'n_estimators': 50}\tMean_score: 0.8550859655759601\tRank: 9\n",
            "Parameters: {'learning_rate': 0.2, 'n_estimators': 100}\tMean_score: 0.8808971375511561\tRank: 4\n",
            "Parameters: {'learning_rate': 0.2, 'n_estimators': 150}\tMean_score: 0.889522615475674\tRank: 2\n",
            "Parameters: {'learning_rate': 0.3, 'n_estimators': 50}\tMean_score: 0.86821454781015\tRank: 7\n",
            "Parameters: {'learning_rate': 0.3, 'n_estimators': 100}\tMean_score: 0.8848932914290243\tRank: 3\n",
            "Parameters: {'learning_rate': 0.3, 'n_estimators': 150}\tMean_score: 0.8917423613025894\tRank: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cascading**"
      ],
      "metadata": {
        "id": "6tAu9F1rvmvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets, assume we are to detect a fraudulent transaction or not \n",
        " \n",
        "Let the dataset be $D_1$ which will be imbalanced, and \n",
        "\n",
        "- $y=1$ for fraudulent transaction\n",
        "- $y =0$ for non fraudulent transaction"
      ],
      "metadata": {
        "id": "o0QwDhMlvpQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a query point $x_q$, \n",
        "- we will pass this point through the first model $M_1$\n",
        "- Model $M_1$ will return the probability of the query point being a fraud\n",
        "\n",
        "\n",
        "Based on probability, we'll split it in 2 parts:\n",
        "- if the probability of $y$ being 1 is extremely low, say $< 0.001$ then \n",
        "    - we consider that as not fraudulent, let this data be $D_1'$.\n",
        "\n",
        "#### What happens to rest of the data? \n",
        "\n",
        "The rest of the points ($D_1-D_1'$) i.e. data with prob. > 0.001 which we are not sure about \n",
        "- will be passed through the next model $M_2$ \n",
        "- Model $M_2$ will be more stricter i.e. it'll penalize more.\n",
        "\n",
        "Again model $M_2$ will split into 2 parts\n",
        "- non fraud (say, $P(y =1 | x_q) < 0.001$)\n",
        "- fraud transac. (p > 0.001)\n",
        "\n",
        "We can again add another model after $M_2$ which will work on same principles\n",
        "\n"
      ],
      "metadata": {
        "id": "i14u9uSDCg9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1TwfqSCDjjS3MsXaBadxNBIvBfF9LQ0JC' >"
      ],
      "metadata": {
        "id": "zfzVddDyS_N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Did you notice the structure of model? \n",
        "We are cascading one model after another.\n",
        "\n",
        "In the first model we are just removing all the genuine customers\n",
        "- in second model, we are trying to find the may be fraudalent points from 2nd data set, \n",
        "\n",
        "we contimue doing this **cascading**\n",
        "\n",
        "Every model is trained on different datasets ($D_n - D_n^1$ )\n",
        "\n",
        "If even after all these models, we are not sure there will be a human at last to verify the same."
      ],
      "metadata": {
        "id": "q98Cc_gUP_Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=16cjT66tLCnFRuGzGPVmw4KUOgz0k85Ot' >\n",
        "\n"
      ],
      "metadata": {
        "id": "Gv0_VBKNTGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZO5770iPbrKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1k_YjnTSbrIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9mG2hu_brGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nta0NawYbrDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}